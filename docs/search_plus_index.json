{"./":{"url":"./","title":"Introduction","keywords":"","body":"Introduction machine learning introduction Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-09-21 11:43:12 "},"math/calculus/":{"url":"math/calculus/","title":"微积分","keywords":"","body":"微积分 Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"math/calculus/taylor.html":{"url":"math/calculus/taylor.html","title":"泰勒公式","keywords":"","body":"泰勒公式 Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"math/linear_algebra.html":{"url":"math/linear_algebra.html","title":"线性代数","keywords":"","body":"线性代数 Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"math/statistics/":{"url":"math/statistics/","title":"概率分布","keywords":"","body":"概率分布 Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"math/statistics/typothetical_test.html":{"url":"math/statistics/typothetical_test.html","title":"假设检验","keywords":"","body":"Typothetical Test 假设检验 一、双边检验 1.1 U检验：\\sigma^2已知，关于\\mu的检验 假设检验 \r H_0: \\mu = \\mu_0, H_1: \\mu \\neq \\mu_0\r 统计量 \r U = \\frac{\\bar{x}-\\mu_0}{\\frac{\\sigma}{\\sqrt{n}}} \\sim N(0,1)\r 拒绝域 根据定义，对于一个给定的置信区间\\alpha，我们可以在正态分布两端取到分位点\\pm u_\\frac{\\alpha}{2},既 \r P\\left\\{ \\left| U \\right| \\geq u_\\frac{\\alpha}{2} \\right\\}= \\alpha \r 如果统计量的值u，\\left| u \\right| \\geq u_\\frac{\\alpha}{2}，则意味着发生了小概率事件，因此原假设H_0为小概率事件，拒绝原假设 故拒绝域为 \r W_1 = \\left \\{ \\left| u \\right| \\geq u_\\frac{\\alpha}{2} \\right \\}\r 1.2 T检验：\\sigma^2未知，关于\\mu的检验 假设检验 \r H_0: \\mu = \\mu_0, H_1: \\mu \\neq \\mu_0\r 统计量 \r T = \\frac{\\bar{x}-\\mu_0}{\\frac{S}{\\sqrt{n}}} \\sim t(n-1)\r S^2为\\alpha^2的无偏估计 拒绝域 \r W_1 = \\left \\{ \\left| t \\right| \\geq t_\\frac{\\alpha}{2}(n-1) \\right \\}\r t分布和正态分布的曲线类似，所以拒绝域的计算方式也类似，不同的是方差未知我们只能用S^2来代替\\alpha^2 1.3 卡方\\chi^2检验：\\mu未知，关于\\sigma^2的检验 假设检验 \r H_0: \\sigma^2 = \\sigma_0^2, H_1: \\sigma^2 \\neq \\sigma_0^2\r 统计量 \r \\chi^2 = \\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi^2(n-1)\r 拒绝域 \r W_1 = \\left \\{ \r \\chi^2 \\leq \\chi^2_{1-\\frac{\\alpha}{2}}(n-1) \r 或者 \r \\chi^2 \\geq \\chi^2_\\frac{\\alpha}{2}(n-1) \r \\right \\}\r 标准卡方分布\\chi^2分布的左右两边不对称，所以将两边分开 二、单边检验 2.1 U单边检验：\\sigma^2已知，关于\\mu的检验 假设检验 \r H_0: \\mu = \\mu_0, H_1: \\mu > \\mu_0 (或 \\mu 统计量 \r U = \\frac{\\bar{x}-\\mu_0}{\\frac{\\sigma}{\\sqrt{n}}} \\sim N(0,1)\r 拒绝域 根据定义，对于一个给定的置信区间\\alpha，我们可以在正态分布取到单个分位点 u_\\alpha,既 \r P\\left\\{ U > u_\\alpha \\right\\}= \\alpha (或P\\left\\{ U 如果统计量的值u，u > u_\\alpha(或u ，则意味着发生了小概率事件，因此原假设H_0为小概率事件，拒绝原假设 故拒绝域为 \r W_1 = \\left \\{ u > u_\\alpha \\right \\}(或W_1 = \\left \\{ u 2.2 T单边检验：\\sigma^2未知，关于\\mu的检验 假设检验 \r H_0: \\mu = \\mu_0, H_1: \\mu > \\mu_0(或\\mu 统计量 \r T = \\frac{\\bar{x}-\\mu_0}{\\frac{S}{\\sqrt{n}}} \\sim t(n-1)\r S^2为\\alpha^2的无偏估计 拒绝域 \r W_1 = \\left \\{ t > t_\\alpha(n-1) \\right \\}(或W_1 = \\left \\{ t t分布和正态分布的曲线类似，所以拒绝域的计算方式也类似，不同的是方差未知我们只能用S^2来代替\\alpha^2 2.3 卡方\\chi^2单边检验：\\mu未知，关于\\sigma^2的检验 假设检验 \r H_0: \\sigma^2 = \\sigma_0^2, H_1: \\sigma^2 > \\sigma_0^2(或\\sigma^2 统计量 \r \\chi^2 = \\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi^2(n-1)\r 拒绝域 \r W_1 = \\left \\{ \\chi^2 > \\chi^2_\\alpha(n-1) \\right \\}\r (或W_1 = \\left \\{ \\chi^2 三、两个独立正态分布总体均值与方差的检验 设X_1,X_2,X_3,...X_{n_1}为总体N(\\mu_1, \\sigma_1^2)的样本, 设Y_1,Y_2,Y_3,...Y_{n_2}为总体N(\\mu_2, \\sigma_2^2)的样本 3.1 U检验：\\sigma_1^2, \\sigma_2^2已知，关于\\mu_1, \\mu_2的检验 假设检验 \r H_0: \\mu_1 = \\mu_2, H_1: \\mu_1 \\neq \\mu_2\r 统计量 \r U = \\frac{\\bar{x}-\\bar{y}}{\\sqrt{\\frac{\\sigma_1^2}{n_1}+\\frac{\\sigma_2^2}{n_2}}} \\sim N(0,1)\r 拒绝域 虽然统计量的计算方程变了，但拒绝域形式不变 \r W_1 = \\left \\{ \\left| u \\right| \\geq u_\\frac{\\alpha}{2} \\right \\}\r 3.2 T检验：\\sigma_1^2, \\sigma_2^2未知，但已知\\sigma_1^2 = \\sigma_2^2，关于\\mu_1, \\mu_2的检验 假设检验 \r H_0: \\mu_1 = \\mu_2, H_1: \\mu_1 \\neq \\mu_2\r 统计量 \r T = \\frac{\\bar{x}-\\bar{y}}{S_w\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} \\sim t(n_1+n_2-2) 其中 \r S_w = \\frac{(n_1-1)S_1^2 + (n_2-1)S_2^2}{n_1+n_2-2}\r S^2为\\alpha^2的无偏估计 拒绝域 \r W_1 = \\left \\{ \\left| t \\right| \\geq t_\\frac{\\alpha}{2}(n_1+n_2-2) \\right \\}\r 3.3 T检验：\\sigma_1^2, \\sigma_2^2未知，但已知\\sigma_1^2 \\neq \\sigma_2^2，关于\\mu_1, \\mu_2的检验 假设检验 \r H_0: \\mu_1 = \\mu_2, H_1: \\mu_1 \\neq \\mu_2\r 统计量 \r T = \\frac{\\bar{x}-\\bar{y}}{\\sqrt{\\frac{S_1}{n_1} + \\frac{S_2}{n_2}}} \\sim t(f) 其中 \r f = \\frac{(\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2})^2}{\\frac{(S_1^2/n_1)^2}{n_1-1} + \\frac{(S_2^2/n_2)^2}{n_2-1}}\r S^2为\\alpha^2的无偏估计 拒绝域 \r W_1 = \\left \\{ \\left| t \\right| \\geq t_\\frac{\\alpha}{2}(n_1+n_2-2) \\right \\}\r 3.4 F检验(方差齐性检验)：\\sigma_1^2, \\sigma_2^2, \\mu_1, \\mu_2未知，关于\\sigma_1^2, \\sigma_2^2的检验 从两研究总体中随机抽取样本，要对这两个样本进行比较的时候，首先要判断两总体方差是否相同，即方差齐性。若两总体方差相等，则直接用t检验，若不等，可采用秩和检验等方法 假设检验 \r H_0: \\sigma_1^2 = \\sigma_2^2, H_1: \\sigma_1^2 \\neq \\sigma_2^2\r 统计量 \r F = \\frac{S_1^2}{S_2^2} \\sim F(n_1-1, n_2-1)\r 拒绝域 \r W_1 = \\left \\{ \r f \\geq F_\\frac{\\alpha}{2}(n_1-1, n_2-1)\r 或者\r f \\leq F_{1-\\frac{\\alpha}{2}}(n_1-1, n_2-1) \r \\right \\}\r 四、非参检验 秩和检验 T-检验：平均值的成对二样本检验 假设条件 两个总体配对差值构成的总体服从正态分布 配对差是由总体差随机抽样得来的 数据配对或匹配（重复测量（前/后）） 统计计算 参考文章 正态总体均值与方差的假设检验 【Excel系列】Excel数据分析：假设检验matplotlib库的常用知识 @ WHAT - HOW - WHY@ 不积跬步 - 无以至千里 Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"math/statistics/distribution.html":{"url":"math/statistics/distribution.html","title":"统计分布","keywords":"","body":"Distribution 分布 正态分布 假设检验 参考文章 matplotlib库的常用知识 @ WHAT - HOW - WHY@ 不积跬步 - 无以至千里 Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"machine-learning/supervised-learning/":{"url":"machine-learning/supervised-learning/","title":"有监督学习","keywords":"","body":"有监督学习 Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"machine-learning/supervised-learning/classification/":{"url":"machine-learning/supervised-learning/classification/","title":"分类","keywords":"","body":"分类 Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"machine-learning/supervised-learning/classification/logistic_regression.html":{"url":"machine-learning/supervised-learning/classification/logistic_regression.html","title":"逻辑回归","keywords":"","body":"逻辑回归 一、假设函数 \r h_\\theta(x) = g(\\theta^TX)=\\frac{1}{1+e^{-\\theta^TX}} (Sigmoid函数)\r Sigmoid函数的X取值范围是(-\\infty, +\\infty)，Y的取值范围是(0, 1)， \r \\begin{cases}\r & \\theta^TX小于0 =\\Rightarrow h_\\theta(x) 0 =\\Rightarrow h_\\theta(x) > 0.5 =\\Rightarrow y=1\\\\ \r & \\theta^TX=0 =\\Rightarrow h_\\theta(x) = 0.5 =\\Rightarrow 决策边界\r \\end{cases}\r 二、代价函数 - 最大似然估计 线性回归的代价函数是平方损失函数，将逻辑回归的假设函数代入公式后的损失函数是一个非凸函数，有很多个局部最优解，没有办法快速的获得全局最优解，于是我们就用上了最大似然估计： \r J(\\theta)=\\begin{cases}\r & \\text{ if y=1 then } -y^{(i)}log(h_\\theta(x^{(i)}) \\\\ \r & \\text{ if y=0 then } -(1-y^{(i)})log(1-h_\\theta(x^{(i)})) \r \\end{cases}\r 整合后 \r J(\\theta)=\\frac{1}{m}(-y^{(i)}log(h_\\theta(x^{(i)})) - (1-y^{(i)})log(1-h_\\theta(x^{(i)})))\r 三、目标函数 \r MinJ(\\theta)\r 四、求解目标函数 1、梯度下降 2、正规方程 五、为什么是Sigmoid函数 https://blog.csdn.net/bitcarmanlee/article/details/51154481 https://blog.csdn.net/bitcarmanlee/article/details/51292380 https://blog.csdn.net/baidu_15238925/article/details/81291247 Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-12-07 10:58:58 "},"machine-learning/supervised-learning/classification/softmax.html":{"url":"machine-learning/supervised-learning/classification/softmax.html","title":"Softmax","keywords":"","body":"Softmax Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"machine-learning/supervised-learning/classification/svm.html":{"url":"machine-learning/supervised-learning/classification/svm.html","title":"SVM","keywords":"","body":"SVM Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"machine-learning/supervised-learning/classification/perceptron.html":{"url":"machine-learning/supervised-learning/classification/perceptron.html","title":"感知机","keywords":"","body":"感知机 Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"machine-learning/supervised-learning/classification/knn.html":{"url":"machine-learning/supervised-learning/classification/knn.html","title":"KNN","keywords":"","body":"KNN Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"machine-learning/supervised-learning/regression/":{"url":"machine-learning/supervised-learning/regression/","title":"回归","keywords":"","body":"回归 Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"machine-learning/supervised-learning/regression/linear_regression.html":{"url":"machine-learning/supervised-learning/regression/linear_regression.html","title":"线性回归","keywords":"","body":"Linear Regression (线性回归) 一、假设函数 \r h_\\theta(x) = \\theta^TX = \\sum_{i=0}^{n}\\theta_ix_i = \\theta_0 + \\theta_1x_1 + \\cdots + \\theta_nx_n\r 二、代价函数 - 平方误差函数 \r J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(h(x^{(i)}) - y^{(i)})^2\r 三、目标函数 \r Min J(\\theta)\r 四、求解目标函数 1、梯度下降 伪代码repeat until convergence { \r \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j}J(\\theta_0, \\theta_1, ...)\r (simulate - for j=0,1,2,...)\r \r \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j}\\frac{1}{2m}\\sum_{i=1}^{m}(h(x^{(i)}) - y^{(i)})^2 \r (for j=0,1,2,...)\r \r \\theta_j := \\theta_j - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}(h(x^{(i)}) - y^{(i)})x^{(i)} \r (for j=0,1,2,...)\r } 开始梯度下降之前： 梯度下降对特征的值敏感，不同的特征取值差别太大会影响收敛效率，所以需要对特征进行归一化，如x_1=\\frac{x_1-\\mu_1}{s_1}， 处理后数据集的均值为0，方差为1 初始点不同可能导致的求解的结果不一样 \\alpha表示学习率，学习率太小，则学习时间太长；学习率太大，可能会错过最低点，最终在最低点来回摆动而无法到达最优； 梯度下降ing： 所有的\\theta_j需要同一批次更新，即更新\\theta_3的时候用的不是最新一批的\\theta_1, \\theta_2,而是上一批次的值，只有等到n-1个变量全都更新完后，才使用最新的值去计算下一批； 求解的最低点倒数为0，越接近最低点倒数绝对值越小，所以\\theta_j的变化也就越小 每次迭代一个\\theta_j都需要用全量的数据，消耗资源多 梯度下降后： 绘制梯度下降曲线，横坐标是迭代次数，纵坐标是损失函数的值。正常情况下曲线应该是单调递减，最后趋近稳定 2、正规方程 居于，最优点的斜率应该为0，直接求解最小损失函数对应的\\theta向量值 \r X = \\begin{pmatrix}\r x_0^{(1)} & x_1^{(1)} & \\cdots & x_n^{(1)} \\\\ \r x_0^{(2)} & x_1^{(2)} & \\cdots & x_n^{(2)} \\\\ \r \\cdots & \\cdots & \\cdots & \\cdots \\\\ \r x_0^{(m)} & x_1^{(m)} & \\cdots & x_n^{(m)} \\\\\r \\end{pmatrix}\r 将所有的样本数据转化为一个(m, n+1)维的X记住结论 \r \\theta = (X^TX)^{-1}X^TY\r 注意： (X^TX)^{-1}，(n+1, m)乘(m, n+1)等于(n+1, n+1)维，求这个矩阵的逆不容易，当n很 大的时候，求解非常耗时，时间复杂度为\\Theta(n^3)； 当求解不可逆的时候， 可能是因为x_i和x_j存在线性关系(特征正规化处理)， 或者因为m \\leq n,即样本数量太小(减小n，删除一些特征来处理) 3、梯度下降 VS 正规方程 梯度下降 正规方程 需要选择学习率 不需要学习率 需要很多次的迭代 不需要迭代 特征维度n很大的时候可以工作 n很大的时候，(n,n)矩阵的逆完全算不出来的 五、多项式回归 将原假设函数h_\\theta(x) = \\theta_0 + \\theta_1x_1 + \\cdots + \\theta_nx_n中的x_i改成多项式的形式，比如\\theta_1x_1 + \\theta_2x_2 + \\theta_3x_1x_2 + \\theta_4x_1^2 + \\theta_5x_2^2 注意，多项式回归方程中每一项都需要进行归一化 Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-12-01 10:59:51 "},"machine-learning/supervised-learning/regression/svr.html":{"url":"machine-learning/supervised-learning/regression/svr.html","title":"SVR","keywords":"","body":"SVR Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"machine-learning/supervised-learning/regression/ridge.html":{"url":"machine-learning/supervised-learning/regression/ridge.html","title":"Ridge","keywords":"","body":"Ridge Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"machine-learning/supervised-learning/regression/lasso.html":{"url":"machine-learning/supervised-learning/regression/lasso.html","title":"Lasso","keywords":"","body":"Lasso Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"machine-learning/supervised-learning/integrated-learning/":{"url":"machine-learning/supervised-learning/integrated-learning/","title":"集成学习","keywords":"","body":"集成学习 Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"machine-learning/supervised-learning/integrated-learning/boosting.html":{"url":"machine-learning/supervised-learning/integrated-learning/boosting.html","title":"Boosting","keywords":"","body":"Boosting Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"machine-learning/supervised-learning/integrated-learning/blending.html":{"url":"machine-learning/supervised-learning/integrated-learning/blending.html","title":"Blending","keywords":"","body":"Blending Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"machine-learning/supervised-learning/integrated-learning/tree.html":{"url":"machine-learning/supervised-learning/integrated-learning/tree.html","title":"Tree","keywords":"","body":"Tree Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"machine-learning/supervised-learning/sequence-labeling/":{"url":"machine-learning/supervised-learning/sequence-labeling/","title":"序列标注","keywords":"","body":"序列标注 Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"machine-learning/supervised-learning/sequence-labeling/hmm.html":{"url":"machine-learning/supervised-learning/sequence-labeling/hmm.html","title":"HMM","keywords":"","body":"HMM Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"machine-learning/supervised-learning/sequence-labeling/crf.html":{"url":"machine-learning/supervised-learning/sequence-labeling/crf.html","title":"CRF","keywords":"","body":"CRF Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"machine-learning/supervised-learning/time-series/":{"url":"machine-learning/supervised-learning/time-series/","title":"时间序列","keywords":"","body":"时间序列 Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"machine-learning/supervised-learning/time-series/mema.html":{"url":"machine-learning/supervised-learning/time-series/mema.html","title":"MEMA","keywords":"","body":"MEMA Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"machine-learning/unsupervised-learning/":{"url":"machine-learning/unsupervised-learning/","title":"无监督学习","keywords":"","body":"无监督学习 Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"machine-learning/unsupervised-learning/clustering/":{"url":"machine-learning/unsupervised-learning/clustering/","title":"聚类","keywords":"","body":"聚类 Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"machine-learning/unsupervised-learning/clustering/kmeans.html":{"url":"machine-learning/unsupervised-learning/clustering/kmeans.html","title":"kmeans","keywords":"","body":"kmeans Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"machine-learning/unsupervised-learning/clustering/gmm.html":{"url":"machine-learning/unsupervised-learning/clustering/gmm.html","title":"GMM","keywords":"","body":"GMM Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"machine-learning/unsupervised-learning/dimensionality-reduction/":{"url":"machine-learning/unsupervised-learning/dimensionality-reduction/","title":"降维","keywords":"","body":"降维 Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"machine-learning/unsupervised-learning/dimensionality-reduction/pca.html":{"url":"machine-learning/unsupervised-learning/dimensionality-reduction/pca.html","title":"PCA","keywords":"","body":"PCA Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"machine-learning/unsupervised-learning/dimensionality-reduction/svd.html":{"url":"machine-learning/unsupervised-learning/dimensionality-reduction/svd.html","title":"SVD","keywords":"","body":"SVD Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"machine-learning/unsupervised-learning/dimensionality-reduction/ae.html":{"url":"machine-learning/unsupervised-learning/dimensionality-reduction/ae.html","title":"AE","keywords":"","body":"AE Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"machine-learning/deep-learning/":{"url":"machine-learning/deep-learning/","title":"深度学习","keywords":"","body":"深度学习 Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"machine-learning/reinforcement-learning/":{"url":"machine-learning/reinforcement-learning/","title":"强化学习","keywords":"","body":"强化学习 Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"machine-learning/graph/":{"url":"machine-learning/graph/","title":"图模型","keywords":"","body":"图模型 Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"machine-learning-summing/hypothetical.html":{"url":"machine-learning-summing/hypothetical.html","title":"假设函数","keywords":"","body":"假设函数 Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"machine-learning-summing/lost.html":{"url":"machine-learning-summing/lost.html","title":"损失函数","keywords":"","body":"损失函数 1.0-1损失函数 （0-1 loss function） L(Y,f(X))=\\left\\{\\begin{matrix} 1,& Y \\neq f(X) \\\\ 0,& Y = f(X) \\end{matrix}\\right. 2.平方损失函数（quadratic loss function) L(Y,f(X))=(Y−f(X))^2 3.绝对值损失函数(absolute loss function) L(Y,f(x))=|Y−f(X)| 4.对数损失函数（logarithmic loss function) 或对数似然损失函数(log-likehood loss function) L(Y,P(Y|X))=−logP(Y|X) 逻辑回归中，采用的则是对数损失函数。如果损失函数越小，表示模型越好 Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-12-04 09:30:00 "},"machine-learning-summing/model.html":{"url":"machine-learning-summing/model.html","title":"优化算法","keywords":"","body":"优化算法 Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"data-analysis/feature_engineering.html":{"url":"data-analysis/feature_engineering.html","title":"特征工程","keywords":"","body":"特征工程 Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"data-analysis/visualization.html":{"url":"data-analysis/visualization.html","title":"数据可视化","keywords":"","body":"数据可视化 Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-10-17 13:12:06 "},"thanks.html":{"url":"thanks.html","title":"感谢&参考文章","keywords":"","body":"感谢&参考文章 Copyright (c) 2018 Tencent PRUCE & MG. all right reserved，powered by Gitbooktime： 2018-09-21 11:43:12 "}}